# آموزش دنیای الستیک - قسمت سوم قسمت های مختلف الستیک استک
ویژگی های منحصر به فرد و پردبازده ی استک الستیک چیزی نیست جز ویژگی های اجزای مختلف آن پس لازم است بریم سراغ شناخت یک به یک آنها .


<img width="100%"  alt="db446f7acc590b3a66f6821118f1ad158e3f322b" src="https://github.com/user-attachments/assets/7e4b7645-6cbc-499a-be5b-208efdb095f0" />

---
## اجزای اصلی Elastic Stack چیست؟

 ۱. بخش **Elasticsearch**

هسته‌ی اصلی. یک موتور جستجوی **مبتنی بر Lucene** که داده‌ها را به صورت **داکیومنت** ذخیره می‌کند و از **ایندکس معکوس** برای جستجوی سریع استفاده می‌کند.

 ۲. بخش **Logstash**

یک پردازشگر قدرتمند داده. داده‌ها را از منابع مختلف جمع می‌کند، فیلتر می‌کند و به Elasticsearch می‌فرستد.

 ۳. بخش **Kibana**

ابزار ویژوال‌سازی. با آن می‌توان داشبورد ساخت، لاگ‌ها را جستجو کرد، آمار تحلیل کرد و حتی با آن هشدار تنظیم کرد.

 ۴. بخش **Beats** 

ابزارهای سبک برای ارسال داده از منابع مختلف مثل سیستم‌عامل، اپلیکیشن و غیره.



## معماری Elastic Stack:
```
Beats (جمع‌آوری) → Logstash (پردازش) → Elasticsearch (ذخیره/تحلیل) → Kibana (نمایش)
```

> <img width="100%" alt="1_5mWqMkmUb1Q_46jMhJfrXw" src="https://github.com/user-attachments/assets/e875bd10-581f-4ee2-a433-3e54da210303" />

> <img width="100%" alt="Untitled-2" src="https://github.com/user-attachments/assets/5ba2c404-ddc5-4b9d-9280-389f579f2527" />




> لزوما همیشه داده ها به Logstash نمیروند بعد بروند به ElasticSearch ممکنه خیلی وقتا پیش بیاید و مستقیم از Beat به ElasticSearch بروند.



## 1. بیتس Beats

بخش Beats از این معماری یه مجموعه ابزار سبک‌وزنه که صرفا برای جمع‌آوری داده‌های **خاص** طراحی شده.

مثل یه تیم متخصص که هر کدوم برای یه کار خاصن:

- **فایل بیت Filebeat**: برای جمع‌آوری لاگ‌های فایل.
- **متریک بیت Metricbeat**: برای جمع‌آوری معیارهای سیستم (مثل CPU و RAM).
- **پکت بیت Packetbeat**: برای مانیتورینگ شبکه.

### چرا Beats ؟

چون Logstash می‌تونه داده‌ها رو صرفا جمع کنه ، و Beat هم میتونه اما Beats سبک‌تر و تخصصی‌ترن.
مثلاً Filebeat فقط روی جمع‌آوری لاگ‌های فایل تمرکز داره و منابع کمتری مصرف می‌کنه.
یعنی یه حالت خاص جمع آوری کننده ی تخصصی هستند.


## 2. لاگستش Logstash
این بخش از معماری الستیک در تولید لاگ (مشترک با بیتس) و جمع آوری و پردازش اولیه ی لاگ ها و داده ها کاربرد دارد.

### لاگ‌ها گنجینه‌ی اطلاعاتن. چرا مهمن؟

- **عیب‌یابی**: اگه یه برنامه خراب بشه، لاگ‌ها بهتون می‌گن کجا و چرا.
- **بهبود عملکرد**: می‌فهمید سیستم کجاها کند شده و چطور بهترش کنید.
- **امنیت**: لاگ‌ها نشون می‌دن کی سعی کرده هک کنه یا یه چیز مشکوک شده.
- **تصمیم‌گیری**: با تحلیل لاگ‌ها می‌فهمید کاربرها چیکار می‌کنن و چطور خدماتتون رو بهتر کنید.

#### مثال
اگه یه فروشگاه آنلاین دارید، لاگ‌ها می‌گن چند نفر سبد خریدشون رو ول کردن و نخریدن—این یعنی یه سرنخ برای بهتر کردن سایتتون.

لاگستش Logstash یه ابزاره توی استک Elasticه که کارش جمع‌آوری، پردازش و ارسال داده‌هاست.
>  خود لاگ های خام برای الستیک هم مشکل ساز هستند چون خیلی وقتا خیلی لاگ ها به دردمان نمیخورد ولی تولید میشود ... خیلی وقتا لاگ ها محتوا های شلوغی دارند و بعدا ذخیره و بازیابی آنها سخت خواهد بود.

قبل از Logstash و Beats ، برای جمع‌آوری لاگ‌ها باید خودتون اسکریپت می‌نوشتید.
این اسکریپت‌ها سخت نگه‌داری می‌شدن و هر بار که چیزی عوض می‌شد، باید کلی تغییرشون می‌دادید. Logstash این کار رو استاندارد و ساده کرد.
مثلاً اگه لاگ‌هاتون توی یه فایل متنیه، Logstash می‌تونه خط به خط بخوندشون و به فرمت JSON بفرسته.


**لاگستش مثل یه پستچی باهوشه که:**

**جمع‌آوری**: لاگ‌ها رو از فایل‌ها، سرورها یا هر جای دیگه می‌گیره.
    
**پردازش**: داده‌ها رو مرتب می‌کنه (مثلاً تاریخ و ساعت رو جدا می‌کنه).
    
**ارسال**: به Elasticsearch می‌فرسته تا ذخیره بشن.
    
---


> <img  width="100%" alt="logstash" src="https://github.com/user-attachments/assets/bc2d3d46-f10b-4234-82fd-a3ec2e666b9e" />




> - ابزار Logstash اومد که داده‌ها رو از هر منبعی بگیره، تمیزشون کنه و آماده‌ی استفاده توی Elasticsearch کنه.
> - مثلاً اگه لاگ‌هاتون توی یه فایل متنیه، Logstash می‌تونه خط به خط بخوندشون و به فرمت JSON بفرسته.


### عملکرد Logstash
فرض کن یه فایل لاگ داری که خط‌هاش این شکلی‌ان:
```log file
2025-07-20 15:54:32, user: ali, action: login, status: success
2025-07-20 15:55:10, user: sara, action: logout, status: success
```
می‌خوای این لاگ‌ها رو بفرستی به Elasticsearch برای تحلیل، اما باید مرتب و ساختارمند بشن.

###  چیکار می‌کنه؟

- ورودی (Input): Logstash اون فایل لاگ رو می‌خونه.
- پردازش (Filter): داده‌ها رو تجزیه می‌کنه و به فرمت JSON تبدیلشون می‌کنه. مثلاً تاریخ، کاربر، اکشن و وضعیت رو جدا می‌کنه.
- خروجی (Output): داده‌های مرتب‌شده رو به Elasticsearch می‌فرسته.

### یه نمونه تنظیمات 
فرض کن فایل تنظیمات Logstash این شکلی باشه:
**مثال Pipeline**:

``` Logstash Pipeline
input {  file {    path => "/var/log/myapp.log"  }}
filter {  grok {    match => { "message" => "%{TIMESTAMP_ISO8601:timestamp}, user: %{WORD:user}, action: %{WORD:action}, status: %{WORD:status}" }  }}
output {  elasticsearch {    hosts => ["http://localhost:9200"]    index => "myapp-logs"  }}
```
### چیو به چی تبدیل می‌کنه؟
ورودی (فایل لاگ خام):
```
2025-07-20 15:54:32, user: ali, action: login, status: success
```

### خروجی (داده‌های JSON برای Elasticsearch):
```Json
{
  "timestamp": "2025-07-20 15:54:32",
  "user": "ali",
  "action": "login",
  "status": "success"
}
```
### چجوری کار می‌کنه؟

- لاگستش فایل لاگ رو خط به خط می‌خونه.
- با فیلتر grok، هر خط رو به بخش‌های معنی‌دار (مثل کاربر و اکشن) تجزیه می‌کنه.
- داده‌های مرتب‌شده رو به مقصد (مثل Elasticsearch) می‌فرسته تا بتونی باهاش گزارش بسازی یا تحلیل کنی.

### نتیجه

لاگستش داده‌های شلخته و خام رو می‌گیره، مرتب و ساختارمند می‌کنه، و برای استفاده بعدی (مثل ذخیره یا نمایش) به یه سیستم دیگه می‌فرسته. 

---

## 3.الستیک سرچ Elasticsearch

الستیک‌سرچ یک موتور جستجوی توزیع‌شده و مقیاس‌پذیر است که برای ذخیره، جستجو و تحلیل داده‌های بزرگ (Big Data) در زمان واقعی طراحی شده است. 
این ابزار بر پایه **آپاچی لوسین** (یک کتابخانه جستجوی متن‌باز) ساخته شده و قابلیت‌های پیشرفته‌ای مثل جستجوی تمام‌متن (Full-Text Search)، تحلیل داده‌ها، و مدیریت داده‌های ساختاریافته و بدون ساختار را فراهم می‌کند.

---
#### **کاربردهای اصلی الستیک‌سرچ:**
1. **جستجوی سریع و تمام‌متن**: برای مثال، جستجو در محتوای وب‌سایت‌ها، اسناد، یا لاگ‌ها.
2. **تحلیل داده‌ها**: تحلیل لاگ‌های سرور، متریک‌های سیستم، یا داده‌های تجاری (مثل داشبوردهای تجاری).
3. **ذخیره‌سازی مقیاس‌پذیر**: ذخیره داده‌های حجیم با قابلیت مقیاس‌پذیری افقی (اضافه کردن نودهای جدید به کلاستری از سرورها).
4. **پشتیبانی از داده‌های متنوع**: از داده‌های ساختاریافته (مثل JSON) تا داده‌های بدون ساختار (مثل متن خام).


---
### **ارتباط الستیک‌سرچ با Beats و Logstash**
الستیک‌سرچ به عنوان مخزن مرکزی داده‌ها عمل می‌کند و داده‌ها را از Beats و Logstash دریافت کرده و به Kibana ارائه می‌دهد.

---

#### **مثال جریان داده**:
1. بخش Filebeat لاگ‌های سرور را جمع‌آوری می‌کند.
2. لاگ‌ها به Logstash ارسال می‌شوند.
3. بخش Logstash داده‌ها را پردازش کرده و به فرمت JSON به الستیک‌سرچ می‌فرستد.
4. الستیک‌سرچ داده‌ها را ایندکس می‌کند و برای جستجو/تحلیل آماده می‌کند.
5. الستیک سرچ داده های جیسان رو از طریق api و جیسان در اختیار Kibana میگذارد.
   
---

### **الستیک‌سرچ به Kibana چه می‌دهد؟**
- الستیک‌سرچ داده‌های ایندکس‌شده را به Kibana ارائه می‌دهد تا برای **ویژوال‌سازی** و **تحلیل** استفاده شوند.
- ابزار Kibana از طریق APIهای الستیک‌سرچ به داده‌ها دسترسی پیدا می‌کند و امکاناتی مثل داشبوردهای تعاملی، نمودارها، و جستجوهای گرافیکی را فراهم می‌کند.

---

### **رابطه الستیک‌سرچ با آپاچی لوسین**
آپاچی لوسین یک کتابخانه جستجوی متن‌باز است که هسته اصلی الستیک‌سرچ را تشکیل می‌دهد. لوسین مسئول **ایندکسینگ** و **جستجوی تمام‌متن** است، اما به تنهایی محدودیت‌هایی دارد که الستیک‌سرچ آن‌ها را برطرف کرده است. 

- لوسین یک کتابخانه جاوا است که قابلیت‌های ایندکسینگ و جستجوی متن را فراهم می‌کند.
- برای ایندکس کردن داده‌ها، لوسین اسناد را به فرمت خاصی (Inverted Index) تبدیل می‌کند که امکان جستجوی سریع کلمات کلیدی را فراهم می‌کند.
- لوسین به صورت مستقل کار می‌کند و برای استفاده از آن، باید به صورت دستی کدنویسی کنید (مثلاً مدیریت ایندکس‌ها، ذخیره‌سازی، و جستجو).
  
---
#### **چگونه الستیک‌سرچ روی لوسین ساخته شده؟**
الستیک‌سرچ به عنوان یک **لایه بالاتر** روی لوسین عمل می‌کند و قابلیت‌های زیر را اضافه می‌کند:
1. **معماری توزیع‌شده**: الستیک‌سرچ داده‌ها را بین چندین نود (سرور) تقسیم می‌کند (Sharding) و قابلیت مقیاس‌پذیری افقی را فراهم می‌کند.
2. **رابط کاربری ساده**: الستیک‌سرچ از APIهای RESTful استفاده می‌کند که با HTTP و JSON کار می‌کنند. این باعث می‌شود توسعه‌دهندگان به راحتی بتوانند داده‌ها را ایندکس، جستجو، یا تحلیل کنند بدون نیاز به کدنویسی پیچیده.
3. **مدیریت خودکار ایندکس‌ها**: الستیک‌سرچ فرآیندهای ایندکسینگ، به‌روزرسانی، و حذف ایندکس‌ها را ساده‌تر کرده و نیازی به مدیریت دستی نیست.
4. **پشتیبانی از داده‌های JSON**: الستیک‌سرچ داده‌ها را به صورت JSON ذخیره می‌کند که برای برنامه‌های مدرن بسیار مناسب است.
5. **قابلیت‌های تحلیلی پیشرفته**: الستیک‌سرچ امکاناتی مثل تجمیع (Aggregations) برای تحلیل داده‌ها و داشبوردسازی فراهم می‌کند.
   
---
#### **تفاوت ایندکسینگ در لوسین و الستیک‌سرچ**
- **در لوسین**: ایندکسینگ دستی است. شما باید خودتان داده‌ها را به فرمت خاصی تبدیل کنید، ایندکس‌ها را بسازید، و مدیریت کنید. این کار نیاز به دانش عمیق و کدنویسی دارد.
- **در الستیک‌سرچ**: ایندکسینگ به صورت خودکار و ساده‌تر انجام می‌شود. شما داده‌ها را به صورت JSON از طریق API ارسال می‌کنید، و الستیک‌سرچ خودش فرآیند ایندکسینگ را مدیریت می‌کند.
- همچنین قابلیت‌هایی مثل **Dynamic Mapping** دارد که به طور خودکار ساختار داده‌ها را تشخیص می‌دهد و ایندکس می‌سازد.


---
### **هسته الستیک‌سرچ چیست؟**
هسته الستیک‌سرچ ترکیبی از **آپاچی لوسین** برای ایندکسینگ و جستجو، و **معماری توزیع‌شده** برای مدیریت داده‌ها در مقیاس بزرگ است. اجزای اصلی هسته الستیک‌سرچ عبارتند از:
1. **ایندکس (Index)**: معادل یک دیتابیس در سیستم‌های سنتی. هر ایندکس شامل مجموعه‌ای از اسناد (Documents) است.
2. **اسناد (Documents)**: واحدهای اصلی داده که به صورت JSON ذخیره می‌شوند. هر سند معادل یک ردیف در دیتابیس است.
3. **شاردینگ (Sharding)**: تقسیم داده‌ها به قطعات کوچک‌تر برای توزیع در نودهای مختلف.
4. **رپلیکا (Replica)**: کپی‌های اضافی از شارد‌ها برای افزایش پایداری و تحمل خطا.
5. بخش **APIهای RESTful**: برای تعامل با الستیک‌سرچ از طریق درخواست‌های HTTP.

---

### **ایندکسینگ در الستیک‌سرچ چگونه ساده شده؟**
الستیک‌سرچ ایندکسینگ را به چند روش ساده‌تر کرده است:
1. بخش **Dynamic Mapping**: الستیک‌سرچ به طور خودکار نوع داده‌های ارسالی (مثل رشته، عدد، تاریخ) را تشخیص می‌دهد و نیازی به تعریف دستی ساختار نیست.
2. بخش **APIهای ساده**: با یک درخواست HTTP ساده (مثل POST) می‌توانید داده‌ها را به ایندکس اضافه کنید.
3. **مدیریت توزیع‌شده**: الستیک‌سرچ خودش شاردینگ و توزیع داده‌ها را مدیریت می‌کند، بنابراین نیازی به مدیریت دستی سرورها نیست.
4. **به‌روزرسانی آسان**: می‌توانید اسناد را به راحتی به‌روزرسانی یا حذف کنید بدون نیاز به بازسازی کل ایندکس.

---

#### **مثال ساده ایندکسینگ در الستیک‌سرچ**:
فرض کنید می‌خواهید یک سند JSON را که ممکنه مستقیم از beats یا logstash آمده است را ایندکس کنید:
```json
POST /my_index/_doc/1
{
  "name": "Ali",
  "age": 30,
  "city": "Tehran"
}
```

---
### **1. مفهوم ایندکسینگ در الستیک‌سرچ**
ایندکسینگ در الستیک‌سرچ به معنای تبدیل داده‌های ورودی (مثل سند JSON) به ساختاری است که برای جستجوی سریع و کارآمد مناسب باشد. این ساختار بر پایه **Inverted Index** (شاخص معکوس) است که از آپاچی لوسن (هسته الستیک‌سرچ) به ارث برده شده است.

- **اینورتد ایندکس** یک ساختار داده است که کلمات یا مقادیر موجود در اسناد را به اسناد حاوی آن‌ها نگاشت (Map) می‌کند.
- **اینورتد ایندکس** به جای ذخیره کل سند به صورت خام، کلمات یا مقادیر کلیدی استخراج شده و به همراه شناسه سند ذخیره می‌شوند تا جستجو سریع‌تر شود.


---
### **2. فرآیند ایندکسینگ سند در الستیک‌سرچ**
وقتی سند JSON بالا را به الستیک‌سرچ ارسال می‌کنید، مراحل زیر طی می‌شود:

#### **مرحله 1: دریافت و تجزیه سند**
- الستیک‌سرچ درخواست POST را دریافت می‌کند.
- سند JSON تجزیه (Parse) می‌شود و فیلدهای آن (`name`, `age`, `city`) شناسایی می‌شوند.
- اگر ایندکس `my_index` از قبل وجود نداشته باشد، الستیک‌سرچ آن را به صورت خودکار ایجاد می‌کند (مگر اینکه تنظیمات خاصی برای جلوگیری از این کار اعمال شده باشد).

#### **مرحله 2: Dynamic Mapping (نگاشت پویا)**
- الستیک‌سرچ به طور خودکار نوع داده هر فیلد را تشخیص می‌دهد:
  - `name: "Ali"` → نوع `text` (برای جستجوی تمام‌متن).
  - `age: 30` → نوع `integer` (برای مقادیر عددی).
  - `city: "Tehran"` → نوع `text` یا `keyword` (بسته به تنظیمات پیش‌فرض).
- این فرآیند به نام **Dynamic Mapping** شناخته می‌شود.
-  الستیک‌سرچ یک Mapping (طرح‌واره) برای ایندکس ایجاد می‌کند که مشخص می‌کند هر فیلد چه نوع داده‌ای دارد و چگونه باید ایندکس شود.

**مثال Mapping خودکار:**
```json
{
  "mappings": {
    "properties": {
      "name": { "type": "text" },
      "age": { "type": "integer" },
      "city": { "type": "text" }
    }
  }
}
```

#### **مرحله 3: تجزیه و تحلیل (Analysis)**
- برای فیلدهای متنی (`text`) مثل `name` و `city`، الستیک‌سرچ از **Analyzer** استفاده می‌کند تا متن را به توکن‌های (Tokens) کوچک‌تر تجزیه کند.
- بخش Analyzer شامل سه مرحله است:
  1. مرحله **Character Filtering**: حذف یا تغییر کاراکترهای خاص (مثل علائم نگارشی).
  2. مرحله **Tokenization**: تقسیم متن به کلمات یا توکن‌ها.
  3. مرحله **Token Filtering**: اعمال فیلترهایی مثل تبدیل به حروف کوچک (lowercase) یا حذف کلمات توقف (stop words).
     
- برای مثال، برای فیلد `city: "Tehran"`:
  - توکن‌های تولیدشده: `["tehran"]` (تبدیل به حروف کوچک برای جستجوی غیرحساس به حروف).
  - اگر فیلد `keyword` باشد، مقدار خام (`Tehran`) بدون تجزیه ذخیره می‌شود.

#### **مرحله 4: ساخت Inverted Index**
- الستیک‌سرچ داده‌ها را به ساختار **Inverted Index** تبدیل می‌کند.
- برای سند مثال ما، Inverted Index چیزی شبیه این خواهد بود (به صورت مفهومی):

| Term     | Document ID |
|----------|-------------|
| ali      | 1           |
| tehran   | 1           |
| 30       | 1           |

- هر Term (کلمه یا مقدار) به شناسه سند (در اینجا `_id: 1`) نگاشت می‌شود.
- برای فیلدهای `text`، توکن‌های تجزیه‌شده ذخیره می‌شوند. برای فیلدهای `keyword` یا `integer`، مقادیر خام ذخیره می‌شوند.

#### **مرحله 5: ذخیره‌سازی و توزیع**
- سند در یک **شارد (Shard)** ذخیره می‌شود. الستیک‌سرچ به طور خودکار تصمیم می‌گیرد که سند در کدام شارد ذخیره شود (بر اساس الگوریتم‌های توزیع).
- اگر **رپلیکا (Replica)** تنظیم شده باشد، یک کپی از سند در شاردهای دیگر ذخیره می‌شود تا تحمل خطا و دسترسی‌پذیری بالا تضمین شود.
- سند اصلی (JSON) نیز به صورت خام در فیلد `_source` ذخیره می‌شود تا بعداً قابل بازیابی باشد.
  

---
### **3. ساختار ایندکس‌شده چگونه است؟**
پس از ایندکسینگ، سند در الستیک‌سرچ به این شکل ذخیره می‌شود:

1. **داده خام (Raw Data)**: سند JSON اصلی در فیلد `_source` ذخیره می‌شود:
   ```json
   {
     "_index": "my_index",
     "_id": "1",
     "_source": {
       "name": "Ali",
       "age": 30,
       "city": "Tehran"
     }
   }
   ```

2. به صورت **Inverted Index**: برای جستجو، ساختار زیر (به صورت مفهومی) ایجاد می‌شود:
   - فیلد `name` (text): توکن `ali` به سند با `_id: 1` اشاره می‌کند.
   - فیلد `city` (text): توکن `tehran` به سند با `_id: 1` اشاره می‌کند.
   - فیلد `age` (integer): مقدار `30` به سند با `_id: 1` اشاره می‌کند.

3. به صورت **Metadata**: اطلاعاتی مثل `_index`، `_id`، و نسخه سند (`_version`) برای مدیریت و ردیابی تغییرات ذخیره می‌شود.

---

### **4. آماده‌سازی برای جستجو**
الستیک‌سرچ سند را برای جستجوی سریع آماده می‌کند:
- **جستجوی تمام‌متن**: برای مثال، اگر جستجو کنید `tehran` در فیلد `city`، الستیک‌سرچ به سرعت سند با `_id: 1` را پیدا می‌کند چون توکن `tehran` در Inverted Index موجود است.
- **جستجوی دقیق**: برای فیلدهای `keyword` یا `integer` (مثل `age: 30`)، الستیک‌سرچ می‌تواند مقادیر دقیق را مقایسه کند.
- **تجمیع (Aggregations)**: الستیک‌سرچ می‌تواند تحلیل‌هایی مثل شمارش تعداد سندها با `city: Tehran` یا میانگین `age` را انجام دهد.

**مثال جستجو:**
```json
GET /my_index/_search
{
  "query": {
    "match": {
      "city": "Tehran"
    }
  }
}
```
این درخواست سندی که `city` آن `Tehran` باشد را برمی‌گرداند (حتی اگر حروف کوچک یا بزرگ باشد، به لطف Analyzer).


---

## 4.کیبانا Kibana

- یک ابزار متن‌باز (open-source) برای **تحلیل و نمایش داده‌ها** است که بخشی از **Elastic Stack** محسوب می‌شود.
- این ابزار به شما کمک می‌کند داده‌های ذخیره‌شده در **Elasticsearch** را به‌صورت گرافیکی (مثل نمودار، جدول، داشبورد) ببینید، تحلیل کنید و باهاشون کار کنید.
- یعنی بیشتر در اختیار و نظارت مدیران سیستم است تا درگیر مفاهیم سطح پایین نشوند و مانیتورینگ داده ها به نحو احسن و خوانا صورت بگیرد.


> زبان ساده: فکر کنید Elasticsearch یه انبار بزرگ داده‌هاست؛ Kibana پنجره‌ایه که بهتون اجازه می‌ده ببینید چی داخل انباره و چه داره اتفاق می‌افته.


وظایف کیبانا Kibana:

- **نمایش داده‌ها**: لاگ‌ها رو به شکل نمودار، جدول یا نقشه نشون می‌ده.
- **تحلیل ساده**: با چند کلیک می‌تونید داده‌ها رو بررسی کنید.
- **دسترسی برای همه**: حتی کسایی که فنی نیستن هم می‌تونن ازش استفاده کنن.

مثلاً می‌تونید یه داشبورد بسازید که نشون بده هر ساعت چند تا خطا داشتید—همه‌چیز با رنگ و شکل قشنگ!


## مثال ساده از عملکرد Kibana با لاگ‌ها

فرض کن یه سرور داری که لاگ‌های فعالیت کاربران (مثل ورود به سیستم یا کلیک‌ها) تو Elasticsearch ذخیره می‌شه.
هر لاگ شامل اطلاعاتی مثل **زمان**، **نوع فعالیت** (مثل login) و **کاربر** (مثل user123) است.

### کیبانا چیو از Elasticsearch می‌گیره؟
کیبانا (Kibana) داده‌های خام (مثل لاگ‌های JSON) رو از Elasticsearch می‌خونه. مثلاً یه لاگ می‌تونه این شکلی باشه:
```json
{
  "timestamp": "2025-07-20 13:00:00",
  "user": "user123",
  "action": "login",
  "status": "success"
}
```

### چیکارش می‌کنه؟
1. **جستجو**: تو Kibana می‌تونی سرچ کنی و فقط لاگ‌های مربوط به "login" رو ببینی.
2. **نمایش**: یه داشبورد درست می‌کنی که مثلاً یه نمودار میله‌ای نشون بده چند نفر تو هر ساعت وارد سیستم شدن.
3. **تحلیل**: می‌تونی ببینی آیا ورودهای ناموفق (failed login) تو یه بازه زمانی خاص زیاد شدن یا نه.

**مثال خروجی در Kibana**:

یه نمودار میله‌ای که نشون می‌ده تو ساعت 13:00، تعداد 50 ورود موفق و 5 ورود ناموفق داشتی. اینطوری سریع می‌فهمی کجا مشکلی هست.


> <img width="623" height="433" alt="chart" src="https://github.com/user-attachments/assets/4e56cad6-9bd3-4c74-96ce-a9f5ca755de0" />

---

**نمایی از داشبورد کیبانا (SIEM Dashboard) **

> <img width="100%" alt="siemdashboard" src="https://github.com/user-attachments/assets/5c0526e4-b23d-41b8-b84c-b0b0e156bd3c" />

































